{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from glob import glob\n",
    "import ujson as json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Mexico\n",
      "Save Data After Downloading 1000 Timelines\n"
     ]
    }
   ],
   "source": [
    "country           = 'Mexico'\n",
    "cutoff            = 1000 # Number of Users Timelines Per File\n",
    "path_to_users     = '../data/decahose/parsed/users/'\n",
    "path_to_locations = '../data/decahose/parsed/locations/'\n",
    "path_to_keys      = '../data/keys/'\n",
    "path_to_timelines = '../data/timelines/'+country.lower().replace(' ','-')+'/'\n",
    "os.makedirs(path_to_timelines, exist_ok=True)\n",
    "\n",
    "print('Country:', country)\n",
    "print('Save Data After Downloading',cutoff,'Timelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID : 0 (Default)\n",
      "SLURM_ARRAY_TASK_ID : 0 (Default)\n",
      "SLURM_ARRAY_TASK_COUNT : 1 (Default)\n",
      "SLURM_JOB_CPUS_PER_NODE : 4 (Default)\n"
     ]
    }
   ],
   "source": [
    "def get_env_var(varname,default):\n",
    "    \n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname,':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname,':', var,'(Default)')\n",
    "    return var\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "SLURM_JOB_ID            = get_env_var('SLURM_JOB_ID',0)\n",
    "SLURM_ARRAY_TASK_ID     = get_env_var('SLURM_ARRAY_TASK_ID',0)\n",
    "SLURM_ARRAY_TASK_COUNT  = get_env_var('SLURM_ARRAY_TASK_COUNT',1)\n",
    "SLURM_JOB_CPUS_PER_NODE = get_env_var('SLURM_JOB_CPUS_PER_NODE',mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Environment Variables: # credentials ( 86 ) > # CPU ( 4 )\n",
      "Only Use 4 Credentials\n",
      "../data/keys/spfraib_sentiments-sam3.json\n",
      "../data/keys/spfraib_sentiments-david.json\n",
      "../data/keys/vinnie_api_keys.json\n",
      "../data/keys/gogps-carolina2.json\n"
     ]
    }
   ],
   "source": [
    "def get_key_files(SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT,SLURM_JOB_CPUS_PER_NODE):\n",
    "\n",
    "    # Randomize set of key files using constant seed\n",
    "    np.random.seed(0)\n",
    "    all_key_files = np.random.permutation(glob(path_to_keys+'*.json'))\n",
    "    \n",
    "    # Split file list by node\n",
    "    key_files = np.array_split(all_key_files,SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]\n",
    "    \n",
    "    # Check that node has more CPU than key file \n",
    "    if len(key_files) <= SLURM_JOB_CPUS_PER_NODE:\n",
    "        print('# Credentials Allocated To Node:', len(key_files)) \n",
    "    else:\n",
    "        print('Check Environment Variables: # credentials (',len(key_files),') > # CPU (', SLURM_JOB_CPUS_PER_NODE,')')\n",
    "        print('Only Use', SLURM_JOB_CPUS_PER_NODE, 'Credentials')\n",
    "        key_files = key_files[:SLURM_JOB_CPUS_PER_NODE]\n",
    "        \n",
    "    return key_files\n",
    "\n",
    "key_files = get_key_files(SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT,SLURM_JOB_CPUS_PER_NODE)\n",
    "print('\\n'.join(key_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Users...\n",
      "# Users: 2651\n",
      "First User Id: 3366741245\n",
      "Node\"s # Users: 2651\n",
      "Node\"s First User Id: 3366741245\n",
      "Computing Time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "def get_users(country,SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT):\n",
    "    \n",
    "    users_by_account_location = pd.read_pickle(path_to_users+'users-by-account-location.pkl.xz')\n",
    "    account_locations = pd.read_pickle(path_to_locations+'account-locations-identified.pkl')\n",
    "\n",
    "    all_users = sorted(frozenset(itertools.chain.from_iterable(\n",
    "    users_by_account_location.reindex(\n",
    "    account_locations.loc[\n",
    "    account_locations['country_long']==country,'LOCATION']).dropna().to_list())))\n",
    "    \n",
    "    del users_by_account_location\n",
    "    del account_locations\n",
    "    \n",
    "    # Randomize All Users\n",
    "    np.random.seed(0)\n",
    "    all_users=np.random.permutation(all_users)\n",
    "    \n",
    "    print('# Users:', len(all_users))\n",
    "    print('First User Id:', all_users[0])\n",
    "    \n",
    "    # Split users by node\n",
    "    users=np.array_split(all_users,SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID].copy()\n",
    "    print('Node\"s # Users:', len(users))\n",
    "    print('Node\"s First User Id:', users[0])\n",
    "    \n",
    "    return users\n",
    "    \n",
    "start = timer()\n",
    "print('Import Users...')\n",
    "\n",
    "users = get_users(country,SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT)\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Downloaded Timelines: 4\n"
     ]
    }
   ],
   "source": [
    "# Users Whose Timeline Were Successfully Downloaded\n",
    "def get_succeed(path_to_timelines):\n",
    "    \n",
    "    if not os.path.exists(path_to_timelines+'succeed.txt'):\n",
    "        return set()\n",
    "    else:\n",
    "        succeed = set()\n",
    "        with open(path_to_timelines+'succeed.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                succeed.add(line.strip('\\n').split('\\t')[0])\n",
    "        return set(succeed)\n",
    "\n",
    "succeed = get_succeed(path_to_timelines)\n",
    "print('# Downloaded Timelines:', len(succeed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Missed Timelines: 0\n"
     ]
    }
   ],
   "source": [
    "# Users Whose Timeline Were Successfully Downloaded\n",
    "def get_failed(path_to_timelines):\n",
    "    \n",
    "    if not os.path.exists(path_to_timelines+'failed.txt'):\n",
    "        return set()\n",
    "    else:\n",
    "        failed = set()\n",
    "        with open(path_to_timelines+'failed.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                failed.add(line.strip('\\n').split('\\t')[0])\n",
    "        return set(failed)\n",
    "\n",
    "failed = get_failed(path_to_timelines)\n",
    "print('# Missed Timelines:', len(failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Checked!\n"
     ]
    }
   ],
   "source": [
    "def get_auth(key_file):\n",
    "    \n",
    "    # Import Key\n",
    "    with open(key_file) as f:\n",
    "        key = json.load(f)\n",
    "\n",
    "    # OAuth process, using the keys and tokens\n",
    "    auth = tweepy.OAuthHandler(key['consumer_key'], key['consumer_secret'])\n",
    "    auth.set_access_token(key['access_token'], key['access_token_secret'])\n",
    "\n",
    "    # Creation of the actual interface, using authentication\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "#         print(key_file,\": Authentication OK\")\n",
    "    except:\n",
    "        print(key_file,\": error during authentication\")\n",
    "        sys.exit('Exit')\n",
    "    \n",
    "    return api\n",
    "\n",
    "\n",
    "for key_file in key_files:\n",
    "    get_auth(key_file)\n",
    "print('Credentials Checked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_timeline(user_id,api):\n",
    "    \n",
    "    timeline = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Collect All Status in Timeline\n",
    "        statuses = tweepy.Cursor(\n",
    "        api.user_timeline, \n",
    "        user_id=user_id, \n",
    "        count=3200,\n",
    "        tweet_mode=\"extended\", \n",
    "        include_rts=True).items()\n",
    "        \n",
    "        for status in statuses:\n",
    "            timeline.append(status._json)\n",
    "        del statuses\n",
    "     \n",
    "    except tweepy.error.TweepError as e:\n",
    "        \n",
    "        # Save User and Error Type\n",
    "        with open(path_to_timelines+'failed.txt', 'a', encoding='utf-8') as file:\n",
    "            file.write(user_id+'\\t'+str(e)+'\\n')\n",
    "        \n",
    "    return pd.DataFrame(timeline)\n",
    "\n",
    "# timeline = get_user_timeline('12',get_auth(key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timelines_by_block(index_key):\n",
    "\n",
    "    # Create Access For Block of Users\n",
    "    api = get_auth(key_files[index_key])\n",
    "    \n",
    "    # Select Block of Users\n",
    "    users_block = np.array_split(users,len(key_files))[index_key]\n",
    "    \n",
    "    # Initialize Output File ID\n",
    "    output_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Initialize Data\n",
    "    timelines = pd.DataFrame()\n",
    "    \n",
    "    # Initialize Downloaded Users\n",
    "    downloaded_ids = []\n",
    "    \n",
    "    for i,user_id in enumerate(users_block):\n",
    "        \n",
    "        # Skip Users With Downloaded Timeline\n",
    "        if user_id in succeed or user_id in failed:\n",
    "#             print('Skip:', user_id)\n",
    "            continue\n",
    "           \n",
    "        # Try Downloading Timeline\n",
    "        timeline  = get_user_timeline(user_id,api)\n",
    "        \n",
    "        if not timeline.shape[0]:\n",
    "#             print('Missed:', user_id)\n",
    "            continue\n",
    "            \n",
    "        # Append\n",
    "        timelines = pd.concat([timelines, timeline],sort=False)\n",
    "        downloaded_ids.append(user_id)\n",
    "            \n",
    "        # Save Every <cutoff> timelines including Last\n",
    "        if len(downloaded_ids) == cutoff or user_id == users_block[-1]:\n",
    "            \n",
    "            print('Process', index_key, 'saving', len(downloaded_ids), 'timelines with output id', output_id)\n",
    "            \n",
    "            # Check Memory Usage\n",
    "            pid = os.getpid()\n",
    "            py = psutil.Process(pid)\n",
    "            memoryUse = py.memory_info()[0]/2.**30  # memory use in GB...I think\n",
    "            print('Process', index_key,'memory use:', round(memoryUse,3),'GB', 'after # users:', i)\n",
    "            \n",
    "            # Save to Json\n",
    "            filename = \\\n",
    "            'timelines-'+\\\n",
    "            str(SLURM_JOB_ID)+'-'+\\\n",
    "            str(SLURM_ARRAY_TASK_ID)+'-'+\\\n",
    "            str(index_key)+'-'+\\\n",
    "            str(len(downloaded_ids))+'-'+\\\n",
    "            output_id+'.json.bz2'\n",
    "            \n",
    "            # bz2 splittable for spark\n",
    "            # save as list of dict\n",
    "            # discard index\n",
    "            timelines.to_json(\n",
    "            path_to_timelines+filename,\n",
    "            orient='records',\n",
    "            force_ascii=False,\n",
    "            date_format=None,\n",
    "            double_precision=15)\n",
    "            \n",
    "            # Read Like This\n",
    "            # pd.read_json(path_to_timelines+filename+'.json.bz2',\n",
    "            # orient='records',\n",
    "            # dtype=False,\n",
    "            # convert_dates=False)\n",
    "            \n",
    "            # Save User Id and File In Which Its Timeline Was Saved\n",
    "            with open(path_to_timelines+'succeed.txt', 'a', encoding='utf-8') as file:\n",
    "                for downloaded_id in downloaded_ids:\n",
    "                    file.write(downloaded_id+'\\t'+filename+'\\n')\n",
    "            \n",
    "            # Reset Output File ID\n",
    "            output_id = str(uuid.uuid4())\n",
    "    \n",
    "            # Reset Data and Downloaded Users\n",
    "            del timelines, downloaded_ids\n",
    "            timelines = pd.DataFrame()\n",
    "            downloaded_ids = []\n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Timelines...\n",
      "\n",
      "Process 0 saving 1 timelines with output id 9ea64359-93c7-4c83-bafc-15a7cdba663c\n",
      "Process 0 memory use: 0.175 GB after # users: 0\n",
      "Process 2 saving 1 timelines with output id 5c201455-0b5c-4b24-a7d1-49dfdcd8fcf4\n",
      "Process 2 memory use: 0.178 GB after # users: 0\n",
      "Process 1 saving 1 timelines with output id c91b3e58-d004-4530-b820-1da17eb2b23f\n",
      "Process 1 memory use: 0.202 GB after # users: 0\n",
      "Process 3 saving 1 timelines with output id 1ef9828e-a3c0-4281-8b04-456aecbfbf0a\n",
      "Process 3 memory use: 0.202 GB after # users: 0\n",
      "Computing Time: 15 sec\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "print('Extract Timelines...\\n')\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    \n",
    "    pool.map(get_timelines_by_block, range(len(key_files)))\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1000/110*84)*60*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('text.json.bz2','w') as f:\n",
    "#     jack.to_json(f,\n",
    "#     orient='records',\n",
    "#     force_ascii=False,\n",
    "#     date_format=None,\n",
    "#     double_precision=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('text.json.bz2','r') as f:\n",
    "#     jack2 = pd.read_json(f,\n",
    "#     orient='records',\n",
    "#     dtype=False,\n",
    "#     convert_dates=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
