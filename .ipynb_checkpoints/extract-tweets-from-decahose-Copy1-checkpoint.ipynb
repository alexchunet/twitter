{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import lzma\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Parameters #############################\n",
    "version_tweets = 'v4'\n",
    "\n",
    "#how = 'by-geocoordinates'\n",
    "how = 'by-account-location'\n",
    "\n",
    "# For testing\n",
    "# years = ['2016']\n",
    "# months = ['12']\n",
    "years = [\"%.2d\" % i for i in range(2012,2019)]\n",
    "months = [\"%.2d\" % i for i in range(1,13)]\n",
    "\n",
    "locations = sorted(['SÃ£o Paulo'])\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/decahose/json/tweets.json.2016-12-23.xz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Array of Input Files\n",
    "def get_input_files(year=None,month=None):\n",
    "\n",
    "    # Tweets Stored on Cluster \n",
    "    if os.getcwd() == '/home/sfraiberger/py':\n",
    "        path_to_input_files  = '/net/twitter/gardenhose-data/json/'\n",
    "        \n",
    "    # For testing\n",
    "    elif os.getcwd() == '/Users/samuel.fraiberger/Dropbox/Work/Projects/twitter/ipynb':\n",
    "        path_to_input_files  = '../data/decahose/json/'\n",
    "        \n",
    "    else:\n",
    "        sys.exit('Incorrect Working Directory... Exiting.')\n",
    "    \n",
    "    # Select all files with .xz extensions\n",
    "    input_files = [file for file in os.listdir(path_to_input_files) if file[-3:]=='.xz']\n",
    "    \n",
    "    if year:\n",
    "        input_files = [file for file in input_files if file.replace('.','-').split('-')[2]==year]\n",
    "\n",
    "    if month:\n",
    "        input_files = [file for file in input_files if file.replace('.','-').split('-')[3]==month]\n",
    "\n",
    "    input_files = [path_to_input_files+file for file in input_files]\n",
    "    \n",
    "    # Randomize For Parallel Processing\n",
    "    return list(np.random.permutation(input_files))\n",
    "\n",
    "get_input_files(year=None,month=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/decahose/parsed/tweets/by-account-location/tweets-by-account-location-from-decahose-v4.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Path to Output File\n",
    "def get_output_file(version_tweets,year=None,month=None):\n",
    "    \n",
    "    path_to_output_files = '../data/decahose/parsed/tweets/'+how+'/'\n",
    "    os.makedirs(path_to_output_files, exist_ok=True)\n",
    "    \n",
    "    output_file = 'tweets-'+how+'-from-decahose'\n",
    "    \n",
    "    if year:\n",
    "        output_file += '-year-'+str(year)\n",
    "        \n",
    "    if month:\n",
    "        output_file += '-month-'+str(month)\n",
    "        \n",
    "    output_file+='-'+version_tweets+'.pkl'\n",
    "    \n",
    "    return path_to_output_files+output_file\n",
    "\n",
    "get_output_file(version_tweets,year=None,month=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets_by_geocoordinates(input_file):\n",
    "    \n",
    "    tweets  = []\n",
    "    \n",
    "    columns=[\n",
    "    'TIME',\n",
    "    'ID',\n",
    "    'TEXT',\n",
    "    'EXTENDED TEXT',\n",
    "    'LANG',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'USER ID',\n",
    "    'USER LOCATION',\n",
    "    'USER UTC OFFSET',\n",
    "    'USER TIME ZONE',\n",
    "    'USER DESCRIPTION',\n",
    "    'USER IMAGE URL',\n",
    "    ]\n",
    "\n",
    "    with lzma.open(input_file,'rb') as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            # Only Select Tweets With Geocoordinates (Could Be in the Replies)\n",
    "            if b'\"coordinates\":{' in line:\n",
    "                \n",
    "                # Json Parsing Can Fail\n",
    "                try:\n",
    "                    tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Only Selects If Geocoordinates in the Original Tweet (Not RT etc.)\n",
    "                if tweet.get('coordinates',None):\n",
    "                    \n",
    "                    tweets.append([\n",
    "                    tweet.get('created_at',None),\n",
    "                    tweet.get('id_str',None),\n",
    "                    tweet.get('text',None),\n",
    "                    tweet.get('extended_tweet', {}).get('full_text', None),\n",
    "                    tweet.get('lang',None),\n",
    "                    tweet['coordinates']['coordinates'][1],\n",
    "                    tweet['coordinates']['coordinates'][0], \n",
    "                    tweet.get('user', {}).get('id_str',None),\n",
    "                    tweet.get('user', {}).get('location',None),\n",
    "                    tweet.get('user', {}).get('utc_offset',None),\n",
    "                    tweet.get('user', {}).get('time_zone',None),\n",
    "                    tweet.get('user', {}).get('description',None),\n",
    "                    tweet.get('user', {}).get('profile_image_url',None),\n",
    "                    ])\n",
    "                    \n",
    "    return pd.DataFrame(tweets, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets_by_account_location(input_file):\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    columns = [\n",
    "    'TIME',\n",
    "    'ID',\n",
    "    'TEXT',\n",
    "    'LANG',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "#     'PLACE',\n",
    "    'USER ID',\n",
    "    'USER LOCATION',\n",
    "#     'USER UTC OFFSET',\n",
    "#     'USER TIME ZONE',\n",
    "#     'USER DESCRIPTION',\n",
    "#     'USER IMAGE URL',\n",
    "    ]\n",
    "\n",
    "    with lzma.open(input_file,'rb') as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            if b',\"location\":' in line:\n",
    "\n",
    "                # Json Parsing Can Fail\n",
    "                try:\n",
    "                    # Encoding Seems to Be Automatically Detected. \n",
    "                    tweet = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Self-reported Account Location\n",
    "                location = tweet.get('user', {}).get('location',None)\n",
    "                \n",
    "                if location and location in locations:\n",
    "\n",
    "                    text = tweet.get('text',None)\n",
    "                    extended_text = tweet.get('extended_tweet', {}).get('full_text', None)\n",
    "                    if extended_text:\n",
    "                        text = extended_text\n",
    "\n",
    "                    lat = None\n",
    "                    lon = None\n",
    "                    if tweet.get('coordinates', None):\n",
    "                        lat = tweet['coordinates']['coordinates'][1]\n",
    "                        lon = tweet['coordinates']['coordinates'][0]\n",
    "\n",
    "                    tweets.append([\n",
    "                    pd.to_datetime(tweet.get('created_at',None)),\n",
    "                    tweet.get('id',None),\n",
    "                    text,\n",
    "                    tweet.get('lang',None),\n",
    "                    lat,\n",
    "                    lon,\n",
    "#                     tweet.get('place', None),\n",
    "                    tweet.get('user', {}).get('id',None),\n",
    "                    location,\n",
    "#                     tweet.get('user', {}).get('utc_offset',None),\n",
    "#                     tweet.get('user', {}).get('time_zone',None),\n",
    "#                     tweet.get('user', {}).get('description',None),\n",
    "#                     tweet.get('user', {}).get('profile_image_url',None),\n",
    "                    ])\n",
    "\n",
    "    return pd.DataFrame(tweets, columns=columns).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    for year in years:\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            start = timer()\n",
    "            print('Year:', year)\n",
    "            print('Month:', month)\n",
    "\n",
    "            input_files = get_input_files(year,month)\n",
    "            print('# Input Files:', len(input_files))\n",
    "            \n",
    "            output_file = get_output_file(version_tweets,year,month)\n",
    "            print('Output File:', output_file)\n",
    "\n",
    "            if not len(input_files):\n",
    "                \n",
    "                print('Skipped.')\n",
    "                print()\n",
    "                continue\n",
    "                \n",
    "            if os.path.exists(output_file):\n",
    "                \n",
    "                print('Output File Already Exists.')\n",
    "                print()\n",
    "                continue\n",
    "    \n",
    "            print('Parse Tweets', how.replace('-',' ').title())\n",
    "            with mp.Pool() as pool:\n",
    "                \n",
    "                if how == 'by-geocoordinates':\n",
    "                    \n",
    "                    tweets = pd.concat(\n",
    "                    pool.map(parse_tweets_by_geocoordinates, input_files)).reset_index(drop=True)\n",
    "                    \n",
    "                elif how == 'by-account-location':\n",
    "                    \n",
    "                    tweets = pd.concat(\n",
    "                    pool.map(parse_tweets_by_account_location, input_files)).reset_index(drop=True)\n",
    "                    \n",
    "                else:\n",
    "                    sys.exit('Parsing Error... Exit')\n",
    "                    \n",
    "            print('# Tweets:', tweets.shape[0])\n",
    "\n",
    "            print('Save Tweets...')\n",
    "            tweets.to_pickle(output_file,compression='xz')\n",
    "            del tweets\n",
    "            print('Done!')\n",
    "\n",
    "            end = timer()\n",
    "            print('Computing Time:', round(end - start), 'sec')\n",
    "            print()\n",
    "                \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2016\n",
      "Month: 12\n",
      "# Input Files: 1\n",
      "Output File: ../data/decahose/parsed/tweets/by-account-location/tweets-by-account-location-from-decahose-year-2016-month-12-v4.pkl\n",
      "Parse Tweets By Account Location\n",
      "# Tweets: 1470\n",
      "Save Tweets...\n",
      "Done!\n",
      "Computing Time: 31 sec\n",
      "\n",
      "Total Computing Time: 31 sec\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "end = timer()\n",
    "print('Total Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIME</th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>EXTENDED TEXT</th>\n",
       "      <th>LANG</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>USER ID</th>\n",
       "      <th>USER LOCATION</th>\n",
       "      <th>USER UTC OFFSET</th>\n",
       "      <th>USER TIME ZONE</th>\n",
       "      <th>USER DESCRIPTION</th>\n",
       "      <th>USER IMAGE URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Dec 23 21:10:52 +0000 2016</td>\n",
       "      <td>812405135255158785</td>\n",
       "      <td>#Alleyway hey: #SantaClaus takes #BeverlyHills...</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>34.101555</td>\n",
       "      <td>-118.338292</td>\n",
       "      <td>434359026</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/6148176389...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Dec 23 21:11:09 +0000 2016</td>\n",
       "      <td>812405206583414784</td>\n",
       "      <td>This #job might be a great fit for you: Sr Man...</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>41.270982</td>\n",
       "      <td>-80.780541</td>\n",
       "      <td>100046855</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>-18000.0</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>Follow this account for geo-targeted Hospitali...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7007271713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Dec 23 21:11:18 +0000 2016</td>\n",
       "      <td>812405244298752005</td>\n",
       "      <td>Goodbye blue, welcome back brunette ððð #blue ...</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>52.394000</td>\n",
       "      <td>-0.535000</td>\n",
       "      <td>70506221</td>\n",
       "      <td>Crewe, England</td>\n",
       "      <td>0.0</td>\n",
       "      <td>London</td>\n",
       "      <td>Instagram &amp; Snapchat: msalex0304</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7754360717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Dec 23 21:11:32 +0000 2016</td>\n",
       "      <td>812405303052419072</td>\n",
       "      <td>Want to work at J. Crew? We're #hiring in #Van...</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>49.282729</td>\n",
       "      <td>-123.120738</td>\n",
       "      <td>28474308</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>-18000.0</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>Follow this account for geo-targeted Retail jo...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/6858544067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Dec 23 21:11:33 +0000 2016</td>\n",
       "      <td>812405307213234176</td>\n",
       "      <td>Con mamÃ¡ ð te amo ðð @ San Clemente del TuyÃº -...</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>-36.365330</td>\n",
       "      <td>-56.716019</td>\n",
       "      <td>1155522932</td>\n",
       "      <td>San Clemente del TuyÃº</td>\n",
       "      <td>-10800.0</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Que sea rock !!</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8071016199...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TIME                  ID  \\\n",
       "0  Fri Dec 23 21:10:52 +0000 2016  812405135255158785   \n",
       "1  Fri Dec 23 21:11:09 +0000 2016  812405206583414784   \n",
       "2  Fri Dec 23 21:11:18 +0000 2016  812405244298752005   \n",
       "3  Fri Dec 23 21:11:32 +0000 2016  812405303052419072   \n",
       "4  Fri Dec 23 21:11:33 +0000 2016  812405307213234176   \n",
       "\n",
       "                                                TEXT EXTENDED TEXT LANG  \\\n",
       "0  #Alleyway hey: #SantaClaus takes #BeverlyHills...          None   en   \n",
       "1  This #job might be a great fit for you: Sr Man...          None   en   \n",
       "2  Goodbye blue, welcome back brunette ððð #blue ...          None   en   \n",
       "3  Want to work at J. Crew? We're #hiring in #Van...          None   en   \n",
       "4  Con mamÃ¡ ð te amo ðð @ San Clemente del TuyÃº -...          None   es   \n",
       "\n",
       "         LAT         LON     USER ID          USER LOCATION  USER UTC OFFSET  \\\n",
       "0  34.101555 -118.338292   434359026                   None              NaN   \n",
       "1  41.270982  -80.780541   100046855              Akron, OH         -18000.0   \n",
       "2  52.394000   -0.535000    70506221         Crewe, England              0.0   \n",
       "3  49.282729 -123.120738    28474308              Vancouver         -18000.0   \n",
       "4 -36.365330  -56.716019  1155522932  San Clemente del TuyÃº         -10800.0   \n",
       "\n",
       "               USER TIME ZONE  \\\n",
       "0                        None   \n",
       "1  Eastern Time (US & Canada)   \n",
       "2                      London   \n",
       "3  Eastern Time (US & Canada)   \n",
       "4                Buenos Aires   \n",
       "\n",
       "                                    USER DESCRIPTION  \\\n",
       "0                                               None   \n",
       "1  Follow this account for geo-targeted Hospitali...   \n",
       "2                   Instagram & Snapchat: msalex0304   \n",
       "3  Follow this account for geo-targeted Retail jo...   \n",
       "4                                    Que sea rock !!   \n",
       "\n",
       "                                      USER IMAGE URL  \n",
       "0  http://pbs.twimg.com/profile_images/6148176389...  \n",
       "1  http://pbs.twimg.com/profile_images/7007271713...  \n",
       "2  http://pbs.twimg.com/profile_images/7754360717...  \n",
       "3  http://pbs.twimg.com/profile_images/6858544067...  \n",
       "4  http://pbs.twimg.com/profile_images/8071016199...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\n",
    "'../data/decahose/parsed/tweets/by-geocoordinates/\\\n",
    "tweets-by-geocoordinates-from-decahose-year-2016-month-12-v4.pkl',\n",
    "compression='xz').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
