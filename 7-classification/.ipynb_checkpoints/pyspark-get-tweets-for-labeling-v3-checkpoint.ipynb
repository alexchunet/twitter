{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lower,rand, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: Samuels-MBP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://samuels-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11727f9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Hostname:', socket.gethostname())\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    if 'samuel' in socket.gethostname().lower():\n",
    "        print('Create Local SparkSession')\n",
    "        spark = SparkSession.builder.config(\n",
    "        \"spark.driver.host\", \"localhost\").appName(\n",
    "        \"get-tweets-for-labeling\").getOrCreate()\n",
    "    else:\n",
    "        print('Create Cluster SparkSession')\n",
    "        spark = SparkSession.builder.appName(\n",
    "        \"get-tweets-for-labeling\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n",
      "Path to data: ../../data/classification/US\n"
     ]
    }
   ],
   "source": [
    "country_code = \"US\"\n",
    "print('Country:', country_code)\n",
    "\n",
    "# Local\n",
    "if  'samuel' in socket.gethostname().lower():\n",
    "    path_to_data = os.path.join('../../data/classification',country_code)\n",
    "# Cluster\n",
    "else:\n",
    "    path_to_data = os.path.join('/user/spf248/twitter/data/classification',country_code)\n",
    "print('Path to data:',path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import tweets containing keywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import tweets containing keywords')\n",
    "filtered = spark.read.parquet(os.path.join(path_to_data,'filtered'))\n",
    "filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import random tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import random tweets')\n",
    "random = spark.read.parquet(os.path.join(path_to_data,'random'))\n",
    "random.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, score: float, target: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import scores')\n",
    "\n",
    "schema = StructType([StructField('tweet_id', StringType(), False),\n",
    "                     StructField('score', FloatType(), False),\n",
    "                     StructField('target', StringType(), False)])\n",
    "\n",
    "scores = spark.read.option('header','true').schema(schema).csv(os.path.join(path_to_data,'similarity'))\n",
    "scores.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Duplicated Scores (Random Sample Could Contain Keywords)\n"
     ]
    }
   ],
   "source": [
    "print('Drop Duplicated Scores (Random Sample Could Contain Keywords)')\n",
    "scores = scores.drop_duplicates(subset=['tweet_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "keywords=sorted([keyword for keyword in filtered.columns if keyword not in ['tweet_id','text','keyword']])\n",
    "print('Keywords:\\n')\n",
    "print('\\n'.join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "\n",
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "targets=sorted(scores.select(\"target\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print('Targets:\\n')\n",
    "print('\\n'.join(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Tweets With Their Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Tweets With Their Similarity Scores:\n",
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "print('Merge Tweets With Their Similarity Scores:')\n",
    "for target in targets:\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "    filtered=filtered.join(\n",
    "    scores.filter(scores['target']==target).selectExpr(\n",
    "    \"tweet_id\", \n",
    "    \"score as \"+target.replace(' ','_').replace('?','').lower()),on='tweet_id')\n",
    "    \n",
    "    random=random.join(\n",
    "    scores.filter(scores['target']==target).selectExpr(\n",
    "    \"tweet_id\", \n",
    "    \"score as \"+target.replace(' ','_').replace('?','').lower()),on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample For Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Sample for Labeling\n"
     ]
    }
   ],
   "source": [
    "print('Create Sample for Labeling')\n",
    "\n",
    "schema = StructType([StructField('tweet_id', StringType(), False),\n",
    "                     StructField('text', StringType(), False),\n",
    "                     StructField('keyword', StringType(), False),\n",
    "                     StructField('target', StringType(), False)])\n",
    "\n",
    "tweets_for_labeling = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "tweets_for_labeling.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sampled tweets per group: 100\n"
     ]
    }
   ],
   "source": [
    "n_sample = 100\n",
    "print('# sampled tweets per group:', n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    \n",
    "    print(keyword)\n",
    "    \n",
    "#     # Remove Those Which Have Already Been Sampled\n",
    "#     filtered = filtered.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "    \n",
    "    # Select Tweets Containing Specific Keyword\n",
    "    tmp = filtered.where(filtered[keyword]==True)\n",
    "    \n",
    "    # Take Random Sample of Size n_sample\n",
    "    tmp = tmp.orderBy(rand(seed=0)).limit(n_sample).select('tweet_id','text')\n",
    "    \n",
    "    # Keep Track of Sampling Properties\n",
    "    tmp = tmp.withColumn(\"keyword\",lit(keyword))\n",
    "    tmp = tmp.withColumn(\"target\",lit('random'))\n",
    "    \n",
    "    tweets_for_labeling = tweets_for_labeling.union(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "I lost my job today\n",
      "\n",
      "hired\n",
      "I lost my job today\n",
      "\n",
      "job\n",
      "I lost my job today\n",
      "\n",
      "laid_off\n",
      "I lost my job today\n",
      "\n",
      "position\n",
      "I lost my job today\n",
      "\n",
      "quit\n",
      "I lost my job today\n",
      "\n",
      "unemployed\n",
      "I lost my job today\n",
      "\n",
      "work\n",
      "I lost my job today\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "\n",
    "    print(keyword)\n",
    "    \n",
    "    for target in targets:\n",
    "        \n",
    "        print(target)\n",
    "        \n",
    "#         # Remove Tweets Which Have Already Been Sampled\n",
    "#         filtered = filtered.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "\n",
    "        # Select Tweets Containing Specific Keyword\n",
    "        tmp = filtered.where(filtered[keyword]==True)\n",
    "    \n",
    "        # Take n_sample Tweets Most Similar with Target Sentence\n",
    "        tmp = tmp.sort(col(target.replace(' ','_').replace('?','').lower()).desc()).limit(n_sample).select('tweet_id','text')\n",
    "        \n",
    "        # Keep Track of Sampling Properties\n",
    "        tmp = tmp.withColumn(\"keyword\",lit(keyword))\n",
    "        tmp = tmp.withColumn(\"target\",lit(target.replace(' ','_').replace('?','').lower()))\n",
    "    \n",
    "        tweets_for_labeling = tweets_for_labeling.union(tmp)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "#     # Remove Tweets Which Have Already Been Sampled\n",
    "#     random = random.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "    \n",
    "    # Take n_sample Tweets Most Similar with Target Sentence\n",
    "    tmp = random.sort(col(target.replace(' ','_').replace('?','').lower()).desc()).limit(n_sample).select('tweet_id','text')\n",
    "    \n",
    "    # Keep Track of Sampling Properties\n",
    "    tmp = tmp.withColumn(\"keyword\",lit('random'))\n",
    "    tmp = tmp.withColumn(\"target\",lit(target.replace(' ','_').lower()))\n",
    "\n",
    "    tweets_for_labeling = tweets_for_labeling.union(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o551.parquet.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3332)\n\tat java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)\n\tat java.lang.StringBuilder.append(StringBuilder.java:136)\n\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:210)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:542)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:541)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:541)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2d74dfa67912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Save'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_for_labeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'labeling-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o551.parquet.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3332)\n\tat java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)\n\tat java.lang.StringBuilder.append(StringBuilder.java:136)\n\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:210)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:542)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:541)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:541)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n"
     ]
    }
   ],
   "source": [
    "print('Save')\n",
    "tweets_for_labeling.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'labeling-v3'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
