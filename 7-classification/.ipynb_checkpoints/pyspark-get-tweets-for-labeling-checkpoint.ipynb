{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lower,rand, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: Samuels-MacBook-Pro.local\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.33:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10c5a7990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Hostname:', socket.gethostname())\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    if 'samuel' in socket.gethostname().lower():\n",
    "        print('Create Local SparkSession')\n",
    "        spark = SparkSession.builder.config(\n",
    "        \"spark.driver.host\", \"localhost\").appName(\n",
    "        \"get-tweets-for-labeling\").getOrCreate()\n",
    "    else:\n",
    "        print('Create Cluster SparkSession')\n",
    "        spark = SparkSession.builder.appName(\n",
    "        \"get-tweets-for-labeling\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n",
      "Path to data: ../../data/classification/US\n"
     ]
    }
   ],
   "source": [
    "country_code = \"US\"\n",
    "print('Country:', country_code)\n",
    "\n",
    "# Local\n",
    "if  'samuel' in socket.gethostname().lower():\n",
    "    path_to_data = os.path.join('../../data/classification',country_code)\n",
    "# Cluster\n",
    "else:\n",
    "    path_to_data = os.path.join('/user/spf248/twitter/data/classification',country_code)\n",
    "print('Path to data:',path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import tweets containing keywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import tweets containing keywords')\n",
    "filtered = spark.read.parquet(os.path.join(path_to_data,'filtered'))\n",
    "filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "|          tweet_id|                text|fired|hired|  job|laid_off|position| quit|unemployed| work|keyword|\n",
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "|616716023677358080|RT @HBMONTE: Sorr...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616716984114241536|Top frustrations ...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616718917679972352|Having two jobs n...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616719991191121920|Can you recommend...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616721396559802369|â˜… JOB â˜… #hiring #...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616721396597592064|RT @h0lymal0ley: ...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616721832536707073|#Job #Hershey Cus...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616722025827074048|Guess What Got He...| true|false| true|   false|   false|false|     false|false|   true|\n",
      "|616722281310466048|â˜… JOB â˜… #hiring #...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616723011241050112|Fireworks Fails T...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616723665661636613|Strategic Learnin...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616725112805552128|Hey guys! Based o...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616725611654987777|#Job #Cambridge A...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616725833898696709|Sr. Manager, Exec...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616726303815921664|RT @HaileeSteinfe...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616726450788503552|RT @OSUPOLICE: Do...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616726689544994816|I hate the fact t...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|616726773724622848|See our latest #D...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616726806977077248|â˜… JOB â˜… #hiring #...|false|false| true|   false|   false|false|     false|false|   true|\n",
      "|616727855599341568|@austin_workman n...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import random tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import random tweets')\n",
    "random = spark.read.parquet(os.path.join(path_to_data,'random'))\n",
    "random.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "|          tweet_id|                text|fired|hired|  job|laid_off|position| quit|unemployed| work|keyword|\n",
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "|554436170910691328|RT @Wize_Quotes: ...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554438989349142528|@mobleydick @Emma...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554439949903491073|@Muslims_USA @net...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554440289465532416|@brintown time to...|false|false|false|   false|   false| true|     false|false|   true|\n",
      "|554441346732544000|ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹?...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554443070431703041|Put my mouth on t...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554444458717351936|RT @weknowwhatsbe...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554444622496550913|@tvtagGalavant @G...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554445545193082880|RT @_ah_its_cassi...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554446384104214528|RT @CocoaLocaa: b...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554446627088654336|RT @Alinas2high: ...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554447579518230528|@Lance210 @MAGCON...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554447709428785153|RT @VinceBAries: ...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554447772263256064|The Barn Collecti...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554449122866913280|JEREMY RENNER FUU...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554451094051770368|Cute gay boys wor...|false|false|false|   false|   false|false|     false| true|   true|\n",
      "|554451672907649024|RT @engadget: Fle...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554453753517338624|I just love this ...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554454521066188800|RT @Salon: The ma...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "|554454768349765632|@GrinandBarrick T...|false|false|false|   false|   false|false|     false|false|  false|\n",
      "+------------------+--------------------+-----+-----+-----+--------+--------+-----+----------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, score: float, target: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import scores')\n",
    "\n",
    "schema = StructType([StructField('tweet_id', StringType(), False),\n",
    "                     StructField('score', FloatType(), False),\n",
    "                     StructField('target', StringType(), False)])\n",
    "\n",
    "scores = spark.read.option('header','true').schema(schema).csv(os.path.join(path_to_data,'similarity'))\n",
    "# scores = spark.read.option('header','true').schema(schema).csv(os.path.join(path_to_data,'similarity','target-*-partition-0.csv'))\n",
    "scores.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Duplicated Scores (Random Sample Could Contain Keywords)\n"
     ]
    }
   ],
   "source": [
    "print('Drop Duplicated Scores (Random Sample Could Contain Keywords)')\n",
    "scores = scores.drop_duplicates(subset=['tweet_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+\n",
      "|           tweet_id|       score|             target|\n",
      "+-------------------+------------+-------------------+\n",
      "|1052224759289638913|  0.99993515|I lost my job today|\n",
      "|1010299704473616386|    0.997035|I lost my job today|\n",
      "|1076337943013404674|  0.99941576|I lost my job today|\n",
      "|1031567199725658112|   0.9941824|I lost my job today|\n",
      "|1056879962764959744|   0.9997024|I lost my job today|\n",
      "|1057680248546840576|  0.99911946|I lost my job today|\n",
      "|1060564260386938885|  0.99918884|I lost my job today|\n",
      "|1030485111236636672|  0.99636996|I lost my job today|\n",
      "|1021533103339143168|  0.99590516|I lost my job today|\n",
      "|1028399598405144577|   0.9998903|I lost my job today|\n",
      "|1033797306959781891|   0.9929697|I lost my job today|\n",
      "|1052557329839923201|   0.9927331|I lost my job today|\n",
      "|1024405878621921281|  0.99839586|I lost my job today|\n",
      "|1031664490809622529|  0.97725874|I lost my job today|\n",
      "|1034964594333306880|   0.9918081|I lost my job today|\n",
      "|1057663664260558854|   0.8601694|I lost my job today|\n",
      "|1051495231399620608|  0.98760045|I lost my job today|\n",
      "|1037277606201241600|   0.9970795|I lost my job today|\n",
      "|1000101300405751809|0.0021098293|I lost my job today|\n",
      "|1022607289763524608|  0.99973816|I lost my job today|\n",
      "+-------------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "keywords=sorted([keyword for keyword in filtered.columns if keyword not in ['tweet_id','text','keyword']])\n",
    "print('Keywords:\\n')\n",
    "print('\\n'.join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "\n",
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "targets=sorted(scores.select(\"target\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print('Targets:\\n')\n",
    "print('\\n'.join(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Sample for Labeling\n"
     ]
    }
   ],
   "source": [
    "print('Create Sample for Labeling')\n",
    "\n",
    "schema = StructType([StructField('tweet_id', StringType(), False),\n",
    "                     StructField('text', StringType(), False),\n",
    "                     StructField('keyword', StringType(), False),\n",
    "                     StructField('target', StringType(), False)])\n",
    "\n",
    "tweets_for_labeling = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "# tweets_for_labeling.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sampled tweets per group: 100\n"
     ]
    }
   ],
   "source": [
    "n_sample = 100\n",
    "print('# sampled tweets per group:', n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    \n",
    "    print(keyword)\n",
    "    \n",
    "    # Select Tweets Containing Specific Keyword\n",
    "    tmp = filtered.where(filtered[keyword]==True)\n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Remove Those Which Have Already Been Sampled\n",
    "    tmp = tmp.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Take Random Sample of Size n_sample\n",
    "    tmp = tmp.orderBy(rand(seed=0)).limit(n_sample).select('tweet_id','text')\n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Keep Track of Sampling Properties\n",
    "    tmp = tmp.withColumn(\"keyword\",lit(keyword.replace('_',' ')))\n",
    "    tmp = tmp.withColumn(\"target\",lit('random'))\n",
    "    \n",
    "    tweets_for_labeling = tweets_for_labeling.union(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "I lost my job today\n",
      "\n",
      "hired\n",
      "I lost my job today\n",
      "\n",
      "job\n",
      "I lost my job today\n",
      "\n",
      "laid_off\n",
      "I lost my job today\n",
      "\n",
      "position\n",
      "I lost my job today\n",
      "\n",
      "quit\n",
      "I lost my job today\n",
      "\n",
      "unemployed\n",
      "I lost my job today\n",
      "\n",
      "work\n",
      "I lost my job today\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "\n",
    "    print(keyword)\n",
    "    \n",
    "    for target in targets:\n",
    "        \n",
    "        print(target)\n",
    "        \n",
    "        # Select Tweets Containing Specific Keyword\n",
    "        tmp = filtered.where(filtered[keyword]==True)\n",
    "#         print('# Tweets:',tmp.count())\n",
    "    \n",
    "        # Merge With Similarity Scores\n",
    "        tmp = tmp.join(scores.filter(scores['target']==target),on='tweet_id')\n",
    "#         print('# Tweets:',tmp.count())\n",
    "        \n",
    "        # Remove Tweets Which Have Already Been Sampled\n",
    "        tmp = tmp.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "#         print('# Tweets:',tmp.count())\n",
    "        \n",
    "        # Take n_sample Tweets Most Similar with Target Sentence\n",
    "        tmp = tmp.sort(col('score').desc()).limit(n_sample).select('tweet_id','text')\n",
    "#         print('# Tweets:',tmp.count())\n",
    "        \n",
    "        # Keep Track of Sampling Properties\n",
    "        tmp = tmp.withColumn(\"keyword\",lit(keyword.replace('_',' ')))\n",
    "        tmp = tmp.withColumn(\"target\",lit(target))\n",
    "    \n",
    "        tweets_for_labeling = tweets_for_labeling.union(tmp)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "    # Merge Random Tweets With Similarity Scores With Target\n",
    "    tmp=random.join(scores.filter(scores['target']==target),on='tweet_id')\n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Remove Tweets Which Have Already Been Sampled\n",
    "    tmp = tmp.join(tweets_for_labeling.select('tweet_id'),on='tweet_id',how='left_anti')  \n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Take n_sample Tweets Most Similar with Target Sentence\n",
    "    tmp = tmp.sort(col('score').desc()).limit(n_sample).select('tweet_id','text')\n",
    "#     print('# Tweets:',tmp.count())\n",
    "    \n",
    "    # Keep Track of Sampling Properties\n",
    "    tmp = tmp.withColumn(\"keyword\",lit('random'))\n",
    "    tmp = tmp.withColumn(\"target\",lit(target))\n",
    "\n",
    "    tweets_for_labeling = tweets_for_labeling.union(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o587.parquet.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3332)\n\tat java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)\n\tat java.lang.StringBuilder.append(StringBuilder.java:136)\n\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:210)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:553)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3241c9fbfdb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Save'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_for_labeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'labeling'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o587.parquet.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3332)\n\tat java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)\n\tat java.lang.StringBuilder.append(StringBuilder.java:136)\n\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:210)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:553)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:568)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$3.apply(TreeNode.scala:566)\n"
     ]
    }
   ],
   "source": [
    "print('Save')\n",
    "tweets_for_labeling.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'labeling'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
